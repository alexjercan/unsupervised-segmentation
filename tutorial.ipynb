{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled6.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyMZbgTg8F9eZ2M2oFWgnadP",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/alexjercan/unsupervised-segmentation/blob/master/tutorial.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5LwOogFQ68Fs"
      },
      "source": [
        "!pip install matplotlib==3.3.3 albumentations==0.5.2"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sdkJeEG97DNa",
        "outputId": "bbc7f379-676e-4fe6-94a5-66ca226abf20"
      },
      "source": [
        "\n",
        "!git clone https://github.com/alexjercan/unsupervised-segmentation.git\n",
        "%cd unsupervised-segmentation\n",
        "\n",
        "import torch\n",
        "from IPython.display import clear_output\n",
        "\n",
        "clear_output()\n",
        "print('Setup complete. Using torch %s %s' % (torch.__version__, torch.cuda.get_device_properties(0) if torch.cuda.is_available() else 'CPU'))"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Setup complete. Using torch 1.8.1+cu101 _CudaDeviceProperties(name='Tesla T4', major=7, minor=5, total_memory=15109MB, multi_processor_count=40)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V9dZftZn7JQP"
      },
      "source": [
        "# Download model\n",
        "torch.hub.download_url_to_file('https://github.com/alexjercan/unsupervised-segmentation/releases/download/v1.0/normal.pth', 'normal.pth')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NWfj0i83gr2i"
      },
      "source": [
        "# Download dataset\n",
        "torch.hub.download_url_to_file('https://github.com/alexjercan/unsupervised-segmentation/releases/download/v1.0/bdataset_scene.zip', 'tmp.zip')\n",
        "!unzip -q tmp.zip -d ../ && rm tmp.zip"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H1SHebvx7PXX"
      },
      "source": [
        "!bash ./get_bdataset.sh"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kQmTa4fD7SDf"
      },
      "source": [
        "!git pull\n",
        "!python dataset.py\n",
        "!python model.py"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tavX9IuI7b93"
      },
      "source": [
        "import os\n",
        "import re\n",
        "import cv2\n",
        "import torch\n",
        "import torch.optim\n",
        "import albumentations as A\n",
        "import my_albumentations as M\n",
        "\n",
        "from datetime import datetime as dt\n",
        "from albumentations.pytorch.transforms import ToTensorV2\n",
        "from config import DEVICE\n",
        "from model import Model, LossFunction\n",
        "from general import init_weights, load_checkpoint, save_checkpoint\n",
        "from dataset import create_dataloader\n",
        "from metrics import MetricFunction, print_single_error\n",
        "from detect import generatePredictions\n",
        "from train import train_one_epoch\n",
        "from test import run_test\n",
        "from util import save_predictions, plot_predictions\n",
        "from dataset import LoadImages\n",
        "\n",
        "IMAGE_SIZE = 256\n",
        "DATASET_ROOT = \"../bdataset_scene\"\n",
        "TRAIN_JSON_PATH = \"train.json\"\n",
        "TEST_JSON_PATH = \"test.json\"\n",
        "IMAGES = [\n",
        "          {\"image\": \"data/0000.png\", \"depth\": \"data/0000.exr\", \"output\": \"data/0000_out.png\"},\n",
        "          {\"image\": \"data/0001.png\", \"depth\": \"data/0001.exr\", \"output\": \"data/0001_out.png\"}\n",
        "]\n",
        "BATCH_SIZE = 8\n",
        "WORKERS = 8\n",
        "\n",
        "LEARNING_RATE = 1e-4\n",
        "BETAS = [0.9, 0.999]\n",
        "EPS = 1e-8\n",
        "WEIGHT_DECAY = 1e-4\n",
        "\n",
        "MILESTONES = [5, 10, 15]\n",
        "GAMMA = 0.2\n",
        "\n",
        "NUM_EPOCHS = 20\n",
        "OUT_PATH =\"./runs\"\n",
        "LOAD_TRAIN_MODEL = False\n",
        "LOAD_TEST_MODEL = False\n",
        "CHECKPOINT_TRAIN_FILE = \"normal.pth\"\n",
        "CHECKPOINT_TEST_FILE = \"normal.pth\"\n",
        "\n",
        "torch.backends.cudnn.benchmark = True"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hOSJyEh67o3H"
      },
      "source": [
        "train_transform = A.Compose(\n",
        "    [\n",
        "        M.MyRandomResizedCrop(width=IMAGE_SIZE, height=IMAGE_SIZE),\n",
        "        M.MyHorizontalFlip(p=0.5),\n",
        "        M.MyVerticalFlip(p=0.1),\n",
        "        A.OneOf([\n",
        "            A.MotionBlur(p=0.2),\n",
        "            A.MedianBlur(blur_limit=3, p=0.1),\n",
        "            A.Blur(blur_limit=3, p=0.1),\n",
        "        ], p=0.2),\n",
        "        A.OneOf([\n",
        "            M.MyOpticalDistortion(p=0.3),\n",
        "            M.MyGridDistortion(p=0.1),\n",
        "        ], p=0.2),\n",
        "        A.OneOf([\n",
        "            A.IAASharpen(),\n",
        "            A.IAAEmboss(),\n",
        "            A.RandomBrightnessContrast(),\n",
        "        ], p=0.3),\n",
        "        A.Normalize(mean=0, std=1),\n",
        "        M.MyToTensorV2(),\n",
        "    ],\n",
        "    additional_targets={\n",
        "        'normal': 'normal',\n",
        "        'depth': 'depth',\n",
        "    }\n",
        ")\n",
        "\n",
        "test_transform = A.Compose(\n",
        "    [\n",
        "        A.Normalize(mean=0, std=1),\n",
        "        M.MyToTensorV2(),\n",
        "    ],\n",
        "    additional_targets={\n",
        "        'normal': 'normal',\n",
        "        'depth': 'depth',\n",
        "    }\n",
        ")\n",
        "\n",
        "detect_transform = A.Compose(\n",
        "    [\n",
        "        A.LongestMaxSize(max_size=IMAGE_SIZE),\n",
        "        A.PadIfNeeded(min_height=IMAGE_SIZE, min_width=IMAGE_SIZE, border_mode=cv2.BORDER_CONSTANT, value=0),\n",
        "        A.Normalize(mean=0, std=1),\n",
        "        M.MyToTensorV2(),\n",
        "    ],\n",
        "    additional_targets={\n",
        "        'depth' : 'depth',\n",
        "    }\n",
        ")\n",
        "\n",
        "_, train_dataloader = create_dataloader(DATASET_ROOT, TRAIN_JSON_PATH, \n",
        "                                        batch_size=BATCH_SIZE, transform=train_transform, \n",
        "                                        workers=WORKERS, pin_memory=True, shuffle=True)\n",
        "\n",
        "_, test_dataloader = create_dataloader(DATASET_ROOT, TEST_JSON_PATH,\n",
        "                                       batch_size=BATCH_SIZE, transform=test_transform,\n",
        "                                       workers=WORKERS, pin_memory=True, shuffle=False)"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eczWCtWc8EoC"
      },
      "source": [
        "model = Model(num_classes=10, num_layers=2)\n",
        "model.apply(init_weights)\n",
        "solver = torch.optim.Adam(filter(lambda p: p.requires_grad, model.parameters()), \n",
        "                          lr=LEARNING_RATE, betas=BETAS, \n",
        "                          eps=EPS, weight_decay=WEIGHT_DECAY)\n",
        "lr_scheduler = torch.optim.lr_scheduler.MultiStepLR(solver, milestones=MILESTONES, gamma=GAMMA)\n",
        "model = model.to(DEVICE)\n",
        "loss_fn = LossFunction()"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JJWRHEzg8Q5Y"
      },
      "source": [
        "epoch_idx = 0\n",
        "if LOAD_TRAIN_MODEL:\n",
        "    epoch_idx, model = load_checkpoint(model, CHECKPOINT_TRAIN_FILE, DEVICE)\n",
        "\n",
        "model.train()\n",
        "for epoch_idx in range(epoch_idx, NUM_EPOCHS):\n",
        "    metric_fn = MetricFunction(BATCH_SIZE)\n",
        "    train_one_epoch(model, train_dataloader, loss_fn, metric_fn, solver, epoch_idx)\n",
        "    print_single_error(epoch_idx, loss_fn.show(), metric_fn.show())\n",
        "    lr_scheduler.step()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e1pIpUYL8SdQ"
      },
      "source": [
        "if LOAD_TEST_MODEL:\n",
        "    epoch_idx, model = load_checkpoint(model, CHECKPOINT_TEST_FILE, DEVICE)\n",
        "\n",
        "model.eval()\n",
        "metric_fn = MetricFunction(BATCH_SIZE)\n",
        "run_test(model, test_dataloader, loss_fn, metric_fn)\n",
        "print_single_error(epoch_idx, loss_fn.show(), metric_fn.show())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_44gTBbN8W_g"
      },
      "source": [
        "if LOAD_TEST_MODEL:\n",
        "    epoch_idx, model = load_checkpoint(model, CHECKPOINT_TEST_FILE, DEVICE)\n",
        "\n",
        "model.eval()\n",
        "images = LoadImages(IMAGES, transform=detect_transform)\n",
        "for img, predictions, path in generatePredictions(model, images):\n",
        "    plot_predictions([img], predictions, [path])\n",
        "    save_predictions(predictions, [path])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OQ3VY7YG8Y6Y"
      },
      "source": [
        "output_dir = os.path.join(OUT_PATH, re.sub(\"[^0-9a-zA-Z]+\", \"-\", dt.now().isoformat()))\n",
        "\n",
        "save_checkpoint(epoch_idx, model, output_dir)"
      ],
      "execution_count": 35,
      "outputs": []
    }
  ]
}